import pandas as pd
import numpy as np
import yfinance as yf
import requests
import streamlit as st
from fortress_config import TICKER_GROUPS
from datetime import datetime

# --- CONSTANTS ---
FORTRESS_STRESS_WINDOWS = {
    "Global Recession Scare (2020)": ("2020-01-01", "2020-04-30"),
    "Rate Hike Pivot (2022)": ("2022-01-01", "2022-07-31"),
    "Small-Cap Shakeout (2024)": ("2024-02-01", "2024-03-31")
}

@st.cache_data(ttl="1d")
def get_benchmark_data(ticker="^NSEI"):
    """Fetches Benchmark data with multi-index handling for True Alpha"""
    try:
        nifty = yf.download(ticker, period="5y", interval="1d", progress=False)
        if nifty.empty: return pd.DataFrame()
        if isinstance(nifty.columns, pd.MultiIndex):
            nifty.columns = nifty.columns.get_level_values(0)
        nifty['ret'] = nifty['Close'].pct_change()
        # Keep Close for rolling calculations if needed, but ret is sufficient for now
        return nifty[['ret']].dropna()
    except Exception as e:
        return pd.DataFrame()

def detect_integrity_issues(fund_df, benchmark_df, category):
    """Calculates Alpha, Beta, Tracking Error, Sortino, Drawdown, and Capture Ratios."""
    try:
        combined = pd.merge(fund_df[['date', 'ret']], benchmark_df[['ret']],
                             left_on='date', right_index=True, suffixes=('_f', '_b'))

        # Require substantial history for accurate "Fortress" analysis
        if len(combined) < 200: return None

        f_ret, b_ret = combined['ret_f'], combined['ret_b']

        # 1. Capture Ratios (Consistency in Bull/Bear cycles)
        # Maintained as Daily Ratio method as per requirements
        up_mkt = b_ret > 0
        down_mkt = b_ret < 0
        upside_cap = (f_ret[up_mkt].mean() / b_ret[up_mkt].mean()) * 100 if b_ret[up_mkt].mean() != 0 else 100
        downside_cap = (f_ret[down_mkt].mean() / b_ret[down_mkt].mean()) * 100 if b_ret[down_mkt].mean() != 0 else 100

        # 2. Risk Metrics (Beta & Tracking Error)
        covariance = np.cov(f_ret, b_ret)[0][1]
        variance = np.var(b_ret)
        beta = covariance / variance if variance != 0 else 1.0
        tracking_error = (f_ret - b_ret).std() * np.sqrt(252) * 100

        # 3. Rolling Alpha (60-day smoothed)
        # Annualized rolling 60-day Alpha
        alpha = (f_ret - b_ret).rolling(60).mean().mean() * 252 * 100

        # 4. Sortino Ratio (Risk-Free Rate = 6%)
        # Numerator uses CAGR over the period, not annualized mean
        RFR = 0.06
        days = len(f_ret)
        years = days / 252.0
        total_ret_f = (1 + f_ret).prod() - 1
        cagr_f = (1 + total_ret_f) ** (1/years) - 1 if years > 0 else 0

        excess_ret = cagr_f - RFR
        neg_ret = f_ret[f_ret < 0]
        downside_dev = neg_ret.std() * np.sqrt(252)
        sortino = excess_ret / downside_dev if downside_dev > 0 else 0

        # 5. Max Drawdown & Win Rate
        cum = (1 + f_ret).cumprod()
        max_dd = ((cum - cum.cummax()) / cum.cummax()).min() * 100

        # Win Rate: Percentage of rolling 252-day periods where Fund CAGR > Benchmark CAGR
        # Calculate rolling 1-year returns
        rolling_f = (1 + f_ret).rolling(252).apply(np.prod, raw=True) - 1
        rolling_b = (1 + b_ret).rolling(252).apply(np.prod, raw=True) - 1

        # Compare and calculate percentage
        wins = (rolling_f > rolling_b).dropna()
        win_rate = wins.mean() * 100 if not wins.empty else 0.0

        # 6. Style Drift Detection (Tiered Beta Thresholds)
        drift_reasons = []
        if tracking_error > 8.0: drift_reasons.append("High TE")

        beta_drift = False
        if category == "Large Cap" and beta > 1.15: beta_drift = True
        elif category == "Flexi/Multi Cap" and beta > 1.20: beta_drift = True
        elif category == "Mid Cap" and beta > 1.25: beta_drift = True
        elif category == "Small Cap" and beta > 1.45: beta_drift = True # Updated to Source of Truth 1.45

        if beta_drift: drift_reasons.append("Beta Drift")

        # Debt Drift check (handled in calculate_drift_status usually, but adding hook here if metric passed)
        # For detect_integrity_issues, we return raw metrics. The Drift Status is calculated later/overlaid.
        # But `drift` string is returned here. So we should update this logic to be consistent.
        # However, this function might not have 'annual_return' easily accessible without recalc.
        # 'cagr_f' is calculated above.

        # Debt Check: Repo Rate 6.5%, Breach < 5.75%
        is_debt = category in ["Liquid/Overnight", "Ultra Short/Low Duration", "Corporate Bond", "Gilt/Dynamic Bond"]
        if is_debt:
            # cagr_f is a decimal (e.g., 0.06 for 6%)
            cagr_pct = cagr_f * 100
            if cagr_pct < 5.75:
                drift_reasons.append(f"Low Yield ({cagr_pct:.1f}%)")

        if not drift_reasons:
            drift = "âœ… Stable"
        else:
            drift = f"ðŸš¨ {' & '.join(drift_reasons)}"

        return {
            "alpha": alpha, "beta": beta, "te": tracking_error, "sortino": sortino,
            "max_dd": max_dd, "win_rate": win_rate, "drift": drift,
            "upside": upside_cap, "downside": downside_cap, "cagr": cagr_f * 100
        }
    except Exception as e:
        # print(f"Error in detect_integrity_issues: {e}")
        return None

@st.cache_data(ttl="7d")
def discover_funds(limit=None):
    """
    Auto-discovers Direct Growth Funds from mfapi.in.
    Filters: Direct, Growth.
    Includes Debt Keywords and ETFs for Debt.
    """
    try:
        url = "https://api.mfapi.in/mf"
        schemes = requests.get(url).json()

        # Updated Keywords
        equity_keywords = ["flexi", "multi", "large", "mid", "small", "focused", "value", "contra"]
        debt_keywords = ["liquid", "gilt", "bond", "duration", "overnight", "corporate"]
        all_keywords = equity_keywords + debt_keywords

        required = ["direct", "growth"]
        base_exclusions = ["regular", "idcw"]
        # "etf" is conditionally excluded later

        candidates = []
        for s in schemes:
            name = s['schemeName'].lower()

            # Must have ALL required terms
            if not all(req in name for req in required):
                continue

            # Must have AT LEAST ONE keyword
            if not any(k in name for k in all_keywords):
                continue

            # Base Exclusions
            if any(ex in name for ex in base_exclusions):
                continue

            # ETF Logic: Exclude ETFs unless it matches Debt keywords
            is_etf = "etf" in name
            is_debt = any(k in name for k in debt_keywords)

            if is_etf:
                if not is_debt:
                    continue # Skip Equity ETFs
                # Else: Allow Debt ETFs (Liquid BeES etc)

            candidates.append(s)

        if limit:
            return candidates[:limit]
        return candidates
    except: return []

def get_category(scheme_name):
    name = scheme_name.lower()

    # --- DEBT CATEGORIES ---
    if "liquid" in name or "overnight" in name: return "Liquid/Overnight"
    if "ultra short" in name or "low duration" in name or "money market" in name: return "Ultra Short/Low Duration"
    if "corporate bond" in name or "credit risk" in name: return "Corporate Bond"
    if "gilt" in name or "dynamic bond" in name or "constant maturity" in name: return "Gilt/Dynamic Bond"
    # General catch-all for bond/debt if not caught above but clearly debt?
    if "bond" in name or "debt" in name: return "Gilt/Dynamic Bond" # Fallback mapping

    # --- EQUITY CATEGORIES ---
    # Priority 1: Specific Styles
    if "focused" in name: return "Focused"
    if "value" in name or "contra" in name: return "Value/Contra"
    if "elss" in name: return "ELSS"

    # Priority 2: Flexi/Multi (Merged)
    if "flexi" in name or "multi" in name: return "Flexi/Multi Cap"

    # Priority 3: Market Cap (Handle exclusions)
    if "mid" in name and "small" in name: return "Small Cap" # Often Small & Mid -> Small Cap proxy

    if "small" in name: return "Small Cap"
    if "mid" in name: return "Mid Cap"
    if "large" in name: return "Large Cap"

    # Priority 4: Fallback
    return "Flexi/Multi Cap"

def calculate_fortress_score(cat_df):
    """
    Calculates the 0-100 normalized Fortress Score for a category dataframe.
    """
    if cat_df.empty: return cat_df

    df = cat_df.copy()
    if len(df) > 1:
        c_min, c_max = df['Score'].min(), df['Score'].max()
        if c_max != c_min:
            # Min-Max Scaling
            df['Fortress Score'] = ((df['Score'] - c_min) / (c_max - c_min)) * 100
        else:
            df['Fortress Score'] = 50.0
    else:
        df['Fortress Score'] = 100.0

    df['Fortress Score'] = df['Fortress Score'].round(1)
    return df

def calculate_drift_status(row):
    """
    Determines Integrity Badge and Status based on thresholds.
    Returns: (Badge, Status, Drift Score, Message)
    """
    try:
        # Defaults if columns missing
        beta = row.get('beta', row.get('Beta', 0))
        te = row.get('te', row.get('Tracking Error', 0))
        cat = row.get('Category', 'Flexi/Multi Cap')

        # We need CAGR for Debt check.
        # If 'cagr' key exists (added in detect_integrity_issues), use it.
        # Or try to infer from Price history if available?
        # Usually row comes from DB which might not have 'cagr'.
        # But 'Score' calculation used Alpha/Sortino.
        # Let's assume we might lack CAGR in legacy DB rows.
        # However, for Debt, we need it.
        # If missing, we can't flag breach properly unless we have 'Alpha' and Benchmark return?
        # Let's use Alpha + Benchmark Yield approximation if needed, or rely on rows having 'cagr' if freshly scanned.
        # For legacy rows, if 'cagr' is missing, skip debt check or mark unknown.
        cagr = row.get('cagr', None)

        # 1. Debt Logic
        is_debt = cat in ["Liquid/Overnight", "Ultra Short/Low Duration", "Corporate Bond", "Gilt/Dynamic Bond"]
        if is_debt:
            if cagr is not None:
                if cagr < 5.75:
                    return "ðŸš¨", "Critical", 100, f"Yield {cagr:.1f}% < 5.75% (Repo Breach)"
            return "âœ…", "Stable", 0, "Stable"

        # 2. Equity Logic
        # Tiered Beta Thresholds
        beta_limit = 1.15
        if cat == "Flexi/Multi Cap": beta_limit = 1.20
        elif cat == "Mid Cap": beta_limit = 1.25
        elif cat == "Small Cap": beta_limit = 1.45 # Source of Truth

        # Tracking Error Threshold
        # Large Cap > 6.0, Others > 9.0
        te_limit = 6.0 if cat == "Large Cap" else 9.0

        # Scoring
        drift_score = 0
        issues = []

        if beta > beta_limit:
            drift_score += 50
            issues.append(f"Beta {beta:.2f} > {beta_limit}")

        if te > te_limit:
            drift_score += 50
            issues.append(f"TE {te:.1f} > {te_limit}")

        # Classification
        if drift_score >= 100:
            return "ðŸš¨", "Critical", drift_score, f"Critical Drift: {' & '.join(issues)}"
        elif drift_score >= 50:
            return "âš ï¸", "Moderate", drift_score, f"Moderate Drift: {' & '.join(issues)}"
        else:
            return "âœ…", "Stable", 0, "Stable"

    except Exception:
        return "â“", "Unknown", 0, "Data Missing"

def apply_drift_status(df):
    """Applies drift calculation to a dataframe."""
    if df.empty: return df

    results = df.apply(calculate_drift_status, axis=1)
    df['Integrity'] = [r[0] for r in results]
    df['Drift Status'] = [r[1] for r in results]
    df['Drift Score'] = [r[2] for r in results]
    df['Drift Message'] = [r[3] for r in results]
    return df

def fetch_market_data():
    """
    Fetches Nifty 50 close, Sector Returns (1M), and Market Breadth (A/D).
    Returns a dictionary of metrics.
    """
    data = {}

    # 1. Nifty 50 Status
    try:
        nifty = yf.download("^NSEI", period="1d", progress=False)
        if not nifty.empty:
            close_val = nifty['Close'].iloc[-1]
            if isinstance(close_val, pd.Series): close_val = close_val.iloc[0] # Handle multi-col mismatch
            data['nifty_level'] = f"{close_val:,.2f}"
        else:
            data['nifty_level'] = "N/A"
    except:
        data['nifty_level'] = "Unavailable"

    # 2. Sector Rotation (1 Month Return)
    sectors = {
        "Banking": "^NSEBANK",
        "IT": "^CNXIT",
        "Auto": "^CNXAUTO",
        "FMCG": "^CNXFMCG",
        "Infra": "^CNXINFRA"
    }

    sector_perf = {}
    for name, ticker in sectors.items():
        try:
            sec_data = yf.download(ticker, period="1mo", progress=False)
            if not sec_data.empty and len(sec_data) > 1:
                start = sec_data['Close'].iloc[0]
                end = sec_data['Close'].iloc[-1]
                # scalar check
                if hasattr(start, 'iloc'): start = start.iloc[0]
                if hasattr(end, 'iloc'): end = end.iloc[0]

                ret = ((end - start) / start) * 100
                sector_perf[name] = ret
            else:
                sector_perf[name] = None
        except:
            sector_perf[name] = None

    data['sectors'] = sector_perf

    # 3. Market Breadth (Nifty 50 A/D)
    # Using TICKER_GROUPS from config, already imported
    try:
        nifty_tickers = TICKER_GROUPS.get("Nifty 50", [])
        if nifty_tickers:
            # Batch download 2 days
            breadth_df = yf.download(nifty_tickers, period="2d", group_by='ticker', progress=False)

            advances = 0
            declines = 0

            for t in nifty_tickers:
                try:
                    # Check if ticker column exists (top level)
                    if t in breadth_df.columns:
                        closes = breadth_df[t]['Close']
                        if len(closes) >= 2:
                            prev = closes.iloc[-2]
                            curr = closes.iloc[-1]
                            if curr > prev: advances += 1
                            elif curr < prev: declines += 1
                except: continue

            data['advances'] = advances
            data['declines'] = declines
            data['ad_ratio'] = f"{advances}/{declines}" if declines > 0 else "Positive"
        else:
            data['ad_ratio'] = "N/A"
    except:
        data['ad_ratio'] = "Unavailable"

    return data

def generate_health_check_report(current_df, previous_df=None):
    """
    Generates the Markdown report for the Health Check.
    """

    # --- 1. PREP DATA ---
    # Apply Categorization & Drift to Current
    if 'Category' not in current_df.columns:
        current_df['Category'] = current_df['Symbol'].apply(get_category)
    current_df = apply_drift_status(current_df)

    # Previous Data Prep (for Hidden Gem)
    if previous_df is not None and not previous_df.empty:
        if 'Category' not in previous_df.columns:
            previous_df['Category'] = previous_df['Symbol'].apply(get_category)

    # --- 2. CALCULATE SCORES ---
    # Categories to iterate
    # Use unique categories found in current data to be dynamic
    categories = current_df['Category'].unique()

    current_scored = pd.DataFrame()
    for cat in categories:
        sub = current_df[current_df['Category'] == cat]
        sub = calculate_fortress_score(sub)
        current_scored = pd.concat([current_scored, sub])

    previous_scored = pd.DataFrame()
    if previous_df is not None and not previous_df.empty:
        for cat in previous_df['Category'].unique():
            sub = previous_df[previous_df['Category'] == cat]
            sub = calculate_fortress_score(sub)
            previous_scored = pd.concat([previous_scored, sub])

    # --- 3. MARKET DATA ---
    mkt = fetch_market_data()

    # Sector Rotation Logic
    # Find Top performing and Bottom performing sector
    secs = mkt.get('sectors', {})
    valid_secs = {k:v for k,v in secs.items() if v is not None}

    sector_summary = "Sector Data Unavailable"
    if valid_secs:
        sorted_secs = sorted(valid_secs.items(), key=lambda x: x[1], reverse=True)
        top_sec = sorted_secs[0]
        bot_sec = sorted_secs[-1]
        sector_summary = f"Capital is rotating into **{top_sec[0]}** (+{top_sec[1]:.2f}%) and out of **{bot_sec[0]}** ({bot_sec[1]:.2f}%) over the last 30 days."

    # --- 4. CATEGORY LEADERBOARDS ---
    # We want Top Fund from each category
    leaderboard_md = ""
    # Defined order for report if present
    priority_order = ["Large Cap", "Mid Cap", "Small Cap", "Flexi/Multi Cap", "Value/Contra", "Focused", "ELSS",
                      "Liquid/Overnight", "Corporate Bond", "Gilt/Dynamic Bond"]

    # Sort categories by priority then others
    cats_sorted = sorted(current_scored['Category'].unique(), key=lambda x: priority_order.index(x) if x in priority_order else 999)

    for cat in cats_sorted:
        cat_data = current_scored[current_scored['Category'] == cat]
        if not cat_data.empty:
            leader = cat_data.sort_values("Fortress Score", ascending=False).iloc[0]
            # Identify specific strength based on metrics?
            tag = "High Performance"
            if leader.get('Sortino', 0) > 2: tag = "Consistency: High"
            elif leader.get('Upside Cap', 0) > 110: tag = "Upside Potential: Elite"
            elif leader.get('Downside Cap', 100) < 85: tag = "Downside Shield: Strong"

            leaderboard_md += f"ðŸ† **{cat} Leader**: {leader['Symbol']} (Score: {leader['Fortress Score']}) | {tag}\n\n"

    # --- 5. STRATEGY WATCHDOG ---
    stable_count = len(current_scored[current_scored['Drift Status'] == 'Stable'])
    total_audited = len(current_scored)

    stable_pct = (stable_count / total_audited * 100) if total_audited > 0 else 0

    watchdog_md = ""
    # List top critical/moderate
    drifters = current_scored[current_scored['Drift Status'] != 'Stable'].sort_values("Drift Score", ascending=False).head(3)
    if not drifters.empty:
        for _, row in drifters.iterrows():
            icon = "ðŸš¨" if row['Drift Status'] == "Critical" else "âš ï¸"
            watchdog_md += f"{icon} **{row['Drift Status']}**: {{{row['Symbol']}}} â€” {row['Drift Message']}\n\n"

    watchdog_md += f"âœ… **Stable**: {stable_pct:.0f}% of audited funds are maintaining their mandate integrity."

    # --- 6. DISCOVERY OPPORTUNITY ---
    gem_md = "New Audit Cycle: Benchmarking in progress."

    if not previous_scored.empty:
        # Merge current and previous on Symbol
        merged = pd.merge(current_scored[['Symbol', 'Fortress Score']],
                          previous_scored[['Symbol', 'Fortress Score']],
                          on='Symbol', suffixes=('_curr', '_prev'))

        merged['Delta'] = merged['Fortress Score_curr'] - merged['Fortress Score_prev']
        merged = merged.sort_values("Delta", ascending=False)

        if not merged.empty:
            top_gem = merged.iloc[0]
            if top_gem['Delta'] > 0:
                gem_md = f"**Hidden Gem**: {{{top_gem['Symbol']}}} has jumped **+{top_gem['Delta']:.1f} points** in Fortress Score this month."

    # --- 7. COMPILE REPORT ---
    report = f"""
ðŸ›¡ï¸ **Fortress Monthly Health Check: {pd.Timestamp.now().strftime('%B %Y')} Template**
Subject: ðŸ›¡ï¸ Fortress Audit: Monthly Portfolio Integrity & Discovery Report

**1. Executive Market Summary (The Heartbeat)**

*   **Nifty 50 Status**: {mkt['nifty_level']}
*   **Sector Rotation**: {sector_summary}
*   **Market Breadth**: {mkt.get('advances', 0)} Advances / {mkt.get('declines', 0)} Declines (A/D Ratio: {mkt.get('ad_ratio', 'N/A')})

**2. Category Leaderboards (Top Fortress Scores)**

{leaderboard_md}
**3. Strategy Watchdog (Integrity Alerts)**

{watchdog_md}

**4. The "Discovery" Opportunity**

{gem_md}
"""
    return report

# --- QUANT-SUITE ENGINES ---

@st.cache_data(ttl="7d")
def fetch_fund_history(scheme_code):
    """
    Fetches daily NAV history from mfapi.in for backtesting.
    Returns a DataFrame with index 'date' and column 'nav', 'ret'.
    """
    try:
        url = f"https://api.mfapi.in/mf/{scheme_code}"
        resp = requests.get(url)
        if resp.status_code != 200: return pd.DataFrame()

        data = resp.json()
        if not data.get('data'): return pd.DataFrame()

        df = pd.DataFrame(data['data'])
        df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y')
        df['nav'] = df['nav'].astype(float)
        df = df.sort_values('date')
        df = df.set_index('date')

        # Calculate daily returns
        df['ret'] = df['nav'].pct_change()

        return df[['nav', 'ret']]
    except Exception as e:
        return pd.DataFrame()

def calculate_correlation_matrix(selected_funds_map):
    """
    Calculates the correlation matrix of daily returns for selected funds.
    selected_funds_map: dict of {scheme_code: scheme_name} or similar identifier.
    Returns: (correlation_df, high_overlap_pairs)
    """
    if not selected_funds_map: return pd.DataFrame(), []

    # Fetch History for all
    frames = {}
    for code, name in selected_funds_map.items():
        hist = fetch_fund_history(code)
        if not hist.empty:
            frames[name] = hist['ret']

    if not frames: return pd.DataFrame(), []

    # Combine into single DF aligned by date
    combined = pd.DataFrame(frames).dropna()

    if combined.empty: return pd.DataFrame(), []

    # Calculate Correlation
    corr_matrix = combined.corr()

    # Identify High Overlap Pairs (> 0.85)
    # Get upper triangle to avoid duplicates
    mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    high_overlap = []

    # Iterate through upper triangle
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            val = corr_matrix.iloc[i, j]
            if val > 0.85:
                pair = f"{corr_matrix.columns[i]} & {corr_matrix.columns[j]}"
                high_overlap.append((pair, val))

    return corr_matrix, high_overlap

def run_crisis_audit(selected_funds_map):
    """
    Performs the 'Crisis Audit' for specific historical windows.
    selected_funds_map: dict of {scheme_code: scheme_name}
    Returns: List of dictionaries containing audit results per fund + portfolio (equal weighted).
    """
    if not selected_funds_map: return []

    # Fetch History
    frames = {}
    for code, name in selected_funds_map.items():
        hist = fetch_fund_history(code)
        if not hist.empty:
            frames[name] = hist

    if not frames: return []

    results = []

    # Helper for MDD and Recovery
    def calculate_crisis_metrics(series, start, end):
        # Slice data
        window_data = series.loc[start:end]
        if window_data.empty: return "N/A", "N/A"

        # MDD
        cum = (1 + window_data['ret']).cumprod()
        peak = cum.cummax()
        dd = (cum - peak) / peak
        max_dd = dd.min() * 100 # percentage

        # Recovery Time (Days to return to Peak after MDD)
        # This is complex to calculate exactly as requested "to return to previous peaks after a 10% drop".
        # Simplified: Days from Max Drawdown valley to new high.
        # If MDD didn't happen (positive period), Recovery is 0.

        if max_dd >= 0: return f"{max_dd:.2f}%", "0 Days"

        return f"{max_dd:.2f}%", f"{len(window_data)} Days (Window)" # Placeholder for complex recovery logic if needed, but MDD is key.

    # Iterate Windows
    for win_name, (start_str, end_str) in FORTRESS_STRESS_WINDOWS.items():
        start_dt = pd.to_datetime(start_str)
        end_dt = pd.to_datetime(end_str)

        # Check individual funds
        for name, df in frames.items():
            mdd_str, rec_str = calculate_crisis_metrics(df, start_dt, end_dt)
            results.append({
                "Window": win_name,
                "Entity": name,
                "Max Drawdown": mdd_str,
                "Recovery": rec_str # Optional, focusing on MDD as primary
            })

    # Calculate Equal Weighted Portfolio
    # Align all series
    combined_ret = pd.DataFrame({name: df['ret'] for name, df in frames.items()}).dropna()
    if not combined_ret.empty:
        # Create portfolio return (mean of returns)
        combined_ret['Portfolio'] = combined_ret.mean(axis=1)

        # Check Portfolio in windows
        port_series = pd.DataFrame({'ret': combined_ret['Portfolio']})

        for win_name, (start_str, end_str) in FORTRESS_STRESS_WINDOWS.items():
             start_dt = pd.to_datetime(start_str)
             end_dt = pd.to_datetime(end_str)
             mdd_str, rec_str = calculate_crisis_metrics(port_series, start_dt, end_dt)

             results.append({
                "Window": win_name,
                "Entity": "ðŸš¨ BLENDED PORTFOLIO",
                "Max Drawdown": mdd_str,
                "Recovery": rec_str
            })

    return results
